{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNPOWS79j9oF5uBT5u0EMR1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"APTihH5fxRa3"},"outputs":[],"source":["import json, csv, sys, re\n","from pathlib import Path\n","from collections import Counter\n","\n","# ---------- Paths ----------\n","INPUT = \"yelp_academic_reviews_dataset.json\"   # JSONL or JSON array/object\n","CSV_OUT = \"yelp_top2000.csv\"\n","JSONL_OUT = \"yelp_top2000.jsonl\"\n","\n","# ---------- Global caps / targets ----------\n","TARGET_TOTAL = 2000\n","STAR_TARGET = {1: 400, 2: 400, 3: 400, 4: 400, 5: 400}  # star balance (rebalance later if short)\n","\n","# Length buckets (inclusive, non-overlapping)\n","# 50–200, 201–350, 351–500, 501–650, 651–800, 801–950, 951–1100, 1101–1250\n","MIN_WORDS, MAX_WORDS = 50, 1250\n","LENGTH_BUCKETS = [\n","    (50, 200,   \"50-200\"),\n","    (201, 350,  \"201-350\"),\n","    (351, 500,  \"351-500\"),\n","    (501, 650,  \"501-650\"),\n","    (651, 800,  \"651-800\"),\n","    (801, 950,  \"801-950\"),\n","    (951, 1100, \"951-1100\"),\n","    (1101, 1250,\"1101-1250\"),\n","]\n","# Per-bucket caps (these sum to 2000 by default)\n","LENGTH_TARGET = {\n","    \"50-200\": 250,\n","    \"201-350\": 250,\n","    \"351-500\": 250,\n","    \"501-650\": 250,\n","    \"651-800\": 250,\n","    \"801-950\": 250,\n","    \"951-1100\": 250,\n","    \"1101-1250\": 250,\n","}\n","length_counts = {k: 0 for k in LENGTH_TARGET}\n","\n","# Entity dominance caps\n","MAX_PER_BUSINESS = 5\n","MAX_PER_USER = 3\n","\n","# Optional quick test cap (set to an int for fast dry runs, or None to disable)\n","MAX_TOTAL = None  # e.g., 50 for quick tests\n","\n","# ---------- Privacy / quality regex ----------\n","RE_URL = re.compile(r'https?://|www\\.', re.I)\n","RE_EMAIL = re.compile(r'\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b', re.I)\n","RE_PHONE = re.compile(r'(\\+?\\d[\\d\\-\\s()]{7,}\\d)')\n","RE_SSN = re.compile(r'\\b\\d{3}-\\d{2}-\\d{4}\\b')\n","RE_NONWORD_HEAVY = re.compile(r'^[^A-Za-z]+$')\n","\n","# ---------- IO helpers ----------\n","def read_any_json(path: Path):\n","    \"\"\"Read JSONL if possible; else parse as a single JSON (array/object).\"\"\"\n","    items = []\n","    try:\n","        with path.open(\"r\", encoding=\"utf-8\") as f:\n","            for line in f:\n","                s = line.strip()\n","                if not s:\n","                    continue\n","                try:\n","                    items.append(json.loads(s))\n","                except json.JSONDecodeError:\n","                    items = None\n","                    break\n","        if items is not None and items:\n","            return items\n","    except FileNotFoundError:\n","        sys.exit(f\"ERROR: File not found: {path}\")\n","\n","    # Fallback: single JSON value\n","    with path.open(\"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","    if isinstance(data, dict):\n","        # Sometimes wrapped; try to find a record dict\n","        # If you know your structure, customize this\n","        for v in data.values():\n","            if isinstance(v, dict):\n","                return [v]\n","        return [data]\n","    return data\n","\n","# ---------- Text utils ----------\n","def norm_text(t: str) -> str:\n","    return \" \".join(t.split()).strip()\n","\n","def word_count(t: str) -> int:\n","    return len(t.split())\n","\n","def looks_all_caps(t: str) -> bool:\n","    letters = [c for c in t if c.isalpha()]\n","    return bool(letters) and all(c.isupper() for c in letters)\n","\n","def bad_privacy_or_quality(t: str) -> bool:\n","    if RE_URL.search(t) or RE_EMAIL.search(t) or RE_PHONE.search(t) or RE_SSN.search(t):\n","        return True\n","    if looks_all_caps(t):\n","        return True\n","    if RE_NONWORD_HEAVY.match(t):\n","        return True\n","    return False\n","\n","def which_length_bucket(wc: int):\n","    if wc < MIN_WORDS or wc > MAX_WORDS:\n","        return None\n","    for lo, hi, label in LENGTH_BUCKETS:\n","        if lo <= wc <= hi:\n","            return label\n","    return None\n","\n","def score_item(rec, wc):\n","    \"\"\"Heuristic: prefer substance + community endorsements.\"\"\"\n","    useful = int(rec.get(\"useful\", 0) or 0)\n","    funny  = int(rec.get(\"funny\", 0) or 0)\n","    cool   = int(rec.get(\"cool\", 0) or 0)\n","    return wc + 2*useful + funny + cool\n","\n","# ---------- Selection helpers ----------\n","def take_with_caps(candidates, target):\n","    \"\"\"\n","    Select up to 'target' from 'candidates' while respecting:\n","      - per-business and per-user caps (local to this call)\n","      - global length bucket caps in LENGTH_TARGET via length_counts (shared)\n","    \"\"\"\n","    picked, per_biz, per_user = [], Counter(), Counter()\n","    for r in candidates:\n","        b = r.get(\"business_id\")\n","        u = r.get(\"user_id\")\n","        lb = r.get(\"WordBucket\")\n","\n","        # Enforce length-bucket caps (global)\n","        if lb and length_counts[lb] >= LENGTH_TARGET[lb]:\n","            continue\n","        # Enforce entity caps (per call)\n","        if b and per_biz[b] >= MAX_PER_BUSINESS:\n","            continue\n","        if u and per_user[u] >= MAX_PER_USER:\n","            continue\n","\n","        picked.append(r)\n","        if b: per_biz[b] += 1\n","        if u: per_user[u] += 1\n","        if lb: length_counts[lb] += 1\n","\n","        if MAX_TOTAL and (sum(length_counts.values()) >= MAX_TOTAL):\n","            break\n","        if len(picked) >= target:\n","            break\n","    return picked\n","\n","def main():\n","    data = read_any_json(Path(INPUT))\n","    if isinstance(data, dict):\n","        data = [data]\n","\n","    # -------- Pass 1: filter, bucket, score --------\n","    seen_hash = set()\n","    by_star = {1: [], 2: [], 3: [], 4: [], 5: []}\n","\n","    for rec in data:\n","        if not isinstance(rec, dict):\n","            continue\n","\n","        stars = rec.get(\"stars\")\n","        text  = rec.get(\"text\")\n","        if stars is None or text is None:\n","            continue\n","\n","        try:\n","            rating = int(stars)\n","        except Exception:\n","            continue\n","        if rating not in (1,2,3,4,5):\n","            continue\n","\n","        if not isinstance(text, str):\n","            continue\n","        txt = text.strip()\n","        if not txt:\n","            continue\n","\n","        wc = word_count(txt)\n","        length_bucket = which_length_bucket(wc)\n","        if length_bucket is None:\n","            continue  # outside 50–1250 range\n","\n","        if bad_privacy_or_quality(txt):\n","            continue\n","\n","        # Deduplicate by normalized text\n","        key = hash(norm_text(txt).lower())\n","        if key in seen_hash:\n","            continue\n","        seen_hash.add(key)\n","\n","        by_star[rating].append({\n","            \"Rating\": rating,\n","            \"Reviews\": txt,\n","            \"WordCount\": wc,\n","            \"WordBucket\": length_bucket,\n","            \"business_id\": rec.get(\"business_id\"),\n","            \"user_id\": rec.get(\"user_id\"),\n","            \"date\": rec.get(\"date\"),\n","            \"_score\": score_item(rec, wc),\n","        })\n","\n","        # Optional fast stop if using MAX_TOTAL and we've grabbed enough rough candidates\n","        if MAX_TOTAL and sum(len(v) for v in by_star.values()) >= MAX_TOTAL * 4:\n","            # heuristic to avoid reading entire massive files during dry-run\n","            break\n","\n","    # Sort each star bin by score desc\n","    for s in by_star:\n","        by_star[s].sort(key=lambda x: x[\"_score\"], reverse=True)\n","\n","    # -------- Pass 2: select with caps + star & length-bucket limits --------\n","    selected = []\n","    shortfall = 0\n","\n","    # First pass: try to satisfy star targets (and length caps along the way)\n","    for s in (1,2,3,4,5):\n","        target = STAR_TARGET[s]\n","        chunk = take_with_caps(by_star[s], target)\n","        selected.extend(chunk)\n","        if len(chunk) < target:\n","            shortfall += (target - len(chunk))\n","        if MAX_TOTAL and sum(length_counts.values()) >= MAX_TOTAL:\n","            break\n","\n","    # Top-up if we're short overall, still respecting length caps + entity caps\n","    if (not MAX_TOTAL and shortfall > 0) or (MAX_TOTAL and sum(length_counts.values()) < MAX_TOTAL):\n","        remaining = []\n","        used_ids = {id(r) for r in selected}\n","        for s in (1,2,3,4,5):\n","            remaining.extend([r for r in by_star[s] if id(r) not in used_ids])\n","        remaining.sort(key=lambda x: x[\"_score\"], reverse=True)\n","\n","        # Recreate entity caps from current 'selected'\n","        per_biz, per_user = Counter(), Counter()\n","        for r in selected:\n","            if r.get(\"business_id\"): per_biz[r[\"business_id\"]] += 1\n","            if r.get(\"user_id\"):     per_user[r[\"user_id\"]]     += 1\n","\n","        for r in remaining:\n","            if MAX_TOTAL and sum(length_counts.values()) >= MAX_TOTAL:\n","                break\n","            b, u, lb = r.get(\"business_id\"), r.get(\"user_id\"), r.get(\"WordBucket\")\n","            if lb and length_counts[lb] >= LENGTH_TARGET[lb]: continue\n","            if b and per_biz[b] >= MAX_PER_BUSINESS: continue\n","            if u and per_user[u] >= MAX_PER_USER: continue\n","            selected.append(r)\n","            if b: per_biz[b] += 1\n","            if u: per_user[u] += 1\n","            if lb: length_counts[lb] += 1\n","            if not MAX_TOTAL and len(selected) >= TARGET_TOTAL:\n","                break\n","\n","    # Final trim to target totals\n","    if MAX_TOTAL:\n","        # Cut to MAX_TOTAL while preserving the already enforced bucket caps\n","        selected = selected[:MAX_TOTAL]\n","    else:\n","        selected = selected[:TARGET_TOTAL]\n","\n","    if not selected:\n","        sys.exit(\"ERROR: No reviews passed the filters/limits.\")\n","\n","    # -------- Output --------\n","    fields = [\"Rating\",\"Reviews\",\"WordCount\",\"WordBucket\",\"business_id\",\"user_id\",\"date\"]\n","    with open(CSV_OUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n","        w = csv.DictWriter(f, fieldnames=fields)\n","        w.writeheader()\n","        for r in selected:\n","            w.writerow({k: r.get(k) for k in fields})\n","\n","    with open(JSONL_OUT, \"w\", encoding=\"utf-8\") as f:\n","        for r in selected:\n","            out = {k: r.get(k) for k in fields}\n","            f.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n","\n","    # -------- Report --------\n","    print(f\"Selected {len(selected)} reviews → {CSV_OUT}, {JSONL_OUT}\")\n","    star_counts = Counter([r[\"Rating\"] for r in selected])\n","    len_counts  = Counter([r[\"WordBucket\"] for r in selected])\n","    # Sort length buckets in the defined order\n","    len_counts_sorted = {label: len_counts.get(label, 0) for _,_,label in LENGTH_BUCKETS}\n","    print(\"Star distribution:\", dict(star_counts))\n","    print(\"Length-bucket distribution:\", len_counts_sorted)\n","    print(\"Bucket caps used:\", {k: f\"{length_counts[k]}/{LENGTH_TARGET[k]}\" for k in LENGTH_TARGET})\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}]}